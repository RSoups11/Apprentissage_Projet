{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c382f456",
   "metadata": {},
   "source": [
    "## Imports des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0c9b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Outils de validation / découpage de scikit-learn\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,     # découpage hold-out train / test\n",
    "    StratifiedKFold,      # K-fold en respectant les proportions de classes\n",
    "    cross_val_predict     # prédictions en validation croisée (toutes les obs.)\n",
    ")\n",
    "\n",
    "# Prétraitement : standardisation des variables explicatives\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Pipeline : chaîner prétraitement + modèle dans un seul objet\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Classifieurs linéaires vus en cours :\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression  # perceptron + régression logistique\n",
    "from sklearn.svm import LinearSVC                               # SVM linéaire (hyperplan à large marge)\n",
    "\n",
    "# Réseau de neurones (perceptron multicouche / MLP)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Métriques d'évaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,             # probabilité de classification correcte (P_test dans le cours)\n",
    "    classification_report,      # précision / rappel / F1 par classe\n",
    "    confusion_matrix,           # matrice de confusion numérique\n",
    "    ConfusionMatrixDisplay      # outil pratique pour tracer la matrice de confusion\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1f525e",
   "metadata": {},
   "source": [
    "## Chargement des données et découpage hold-out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416cd7ed",
   "metadata": {},
   "source": [
    "### Chargement du fichier de données nettoyées (préparé dans le notebook d'EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c29cfaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total d'exemples : 569\n",
      "Taille ensemble d'entraînement : 455\n",
      "Taille ensemble de test        : 114\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/data_clean.csv\")\n",
    "\n",
    "# Suppression éventuelle d'une colonne d'index technique qui ne doit pas être utilisée comme feature\n",
    "df = df.drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
    "\n",
    "# Séparation entre X : matrice des variables explicatives (toutes les colonnes sauf 'diagnosis'), et y : vecteur des étiquettes (diagnostic 0/1)\n",
    "X = df.drop(columns=[\"diagnosis\"])\n",
    "y = df[\"diagnosis\"]\n",
    "\n",
    "# Découpage hold-out : 80 % des données pour l'entraînement, 20 % pour le test\n",
    "# On stratifie sur y pour conserver la proportion bénin/malin dans les deux sous-échantillons\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,       # environ 20 % pour l'ensemble de test\n",
    "    random_state=0,      # graine pour rendre le découpage reproductible\n",
    "    stratify=y           # stratification sur la variable cible\n",
    ")\n",
    "\n",
    "print(\"Nombre total d'exemples :\", X.shape[0])\n",
    "print(\"Taille ensemble d'entraînement :\", X_train.shape[0])\n",
    "print(\"Taille ensemble de test        :\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f7d66b",
   "metadata": {},
   "source": [
    "## Fonction utilitaire pour analyser la matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7bf62cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrire_matrice_confusion(cm, labels):\n",
    "    \"\"\"\n",
    "    Prend une matrice de confusion 2x2 et affiche pour chaque classe :\n",
    "      - vrais positifs (VP)\n",
    "      - faux positifs (FP)\n",
    "      - faux négatifs (FN)\n",
    "      - vrais négatifs (VN)\n",
    "    L'ordre des labels doit correspondre à l'ordre utilisé dans confusion_matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # On suppose un problème binaire avec labels = [0, 1]\n",
    "    # cm[i, j] = nombre d'exemples de classe vraie i prédits comme j\n",
    "    tn = cm[0, 0]  # vrai négatif  : vrai 0, prédit 0  (tumeur bénigne bien classée)\n",
    "    fp = cm[0, 1]  # faux positif  : vrai 0, prédit 1  (bénigne prédite maligne)\n",
    "    fn = cm[1, 0]  # faux négatif  : vrai 1, prédit 0  (maligne prédite bénigne)\n",
    "    tp = cm[1, 1]  # vrai positif  : vrai 1, prédit 1  (tumeur maligne bien classée)\n",
    "\n",
    "    print(f\"\\nDétail de la matrice de confusion (ordre des classes : {labels})\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(f\"Vrais négatifs  (TN) : {tn}  -> tumeurs bénignes correctement détectées\")\n",
    "    print(f\"Faux positifs   (FP) : {fp}  -> tumeurs bénignes prédites malignes\")\n",
    "    print(f\"Faux négatifs   (FN) : {fn}  -> tumeurs malignes prédites bénignes\")\n",
    "    print(f\"Vrais positifs  (TP) : {tp}  -> tumeurs malignes correctement détectées\")\n",
    "\n",
    "    # Calcul du nombre total de faux positifs / faux négatifs\n",
    "    total_fp = fp\n",
    "    total_fn = fn\n",
    "\n",
    "    print(\"\\nRésumé des erreurs :\")\n",
    "    print(f\"  - Nombre total de faux positifs  : {total_fp}\")\n",
    "    print(f\"  - Nombre total de faux négatifs  : {total_fn}\")\n",
    "\n",
    "    # Selon le contexte médical, les faux négatifs sont les plus graves (cancer non détecté)\n",
    "    if total_fn > total_fp:\n",
    "        print(\"  -> Le modèle commet davantage de faux négatifs que de faux positifs.\")\n",
    "    elif total_fn < total_fp:\n",
    "        print(\"  -> Le modèle commet davantage de faux positifs que de faux négatifs.\")\n",
    "    else:\n",
    "        print(\"  -> Le modèle commet autant de faux positifs que de faux négatifs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caf0baf",
   "metadata": {},
   "source": [
    "## Fonctions d’évaluation (hold-out + validation croisée)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4569078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions d'évaluation (hold-out et CV k-fold)\n",
    "\n",
    "def eval_holdout(model, model_name, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Entraîne un modèle sur l'ensemble d'entraînement (stratégie hold-out)\n",
    "    puis l'évalue sur l'ensemble de test.\n",
    "\n",
    "    Affiche :\n",
    "      - l'accuracy (taux de bonnes classifications)\n",
    "      - le taux d'erreur (= 1 - accuracy)\n",
    "      - le rapport de classification\n",
    "      - la matrice de confusion + son interprétation (FP / FN)\n",
    "    Retourne un dictionnaire avec les métriques principales.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n================= {model_name} — Stratégie hold-out =================\")\n",
    "\n",
    "    # Entraînement du modèle sur les données d'entraînement\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Prédictions du modèle sur les données de test\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Taux de bonnes classifications (accuracy au sens du cours P_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    err = 1.0 - acc   # taux d'erreur empiriquement estimé sur le test\n",
    "\n",
    "    print(f\"\\nAccuracy (test)      : {acc:.4f}\")\n",
    "    print(f\"Taux d'erreur (test) : {err:.4f}\\n\")\n",
    "\n",
    "    # Rapport détaillé : précision, rappel, F1 par classe\n",
    "    print(\"Rapport de classification (test) :\")\n",
    "    print(classification_report(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        target_names=[\"Bénigne (0)\", \"Maligne (1)\"]\n",
    "    ))\n",
    "\n",
    "    # Matrice de confusion (ordre des labels fixé : 0 puis 1)\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "\n",
    "    print(\"Matrice de confusion (test) :\")\n",
    "    print(cm)\n",
    "\n",
    "    # Détail des faux positifs / faux négatifs\n",
    "    decrire_matrice_confusion(cm, labels=[0, 1])\n",
    "\n",
    "    # Affichage graphique de la matrice de confusion\n",
    "    disp = ConfusionMatrixDisplay(\n",
    "        confusion_matrix=cm,\n",
    "        display_labels=[\"Bénigne (0)\", \"Maligne (1)\"]\n",
    "    )\n",
    "    fig, ax = plt.subplots()\n",
    "    disp.plot(ax=ax, values_format=\"d\")\n",
    "    plt.title(f\"Matrice de confusion — {model_name} (hold-out)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Renvoi des métriques principales pour le tableau récapitulatif\n",
    "    return {\n",
    "        \"modèle\": model_name,\n",
    "        \"stratégie\": \"hold-out\",\n",
    "        \"accuracy\": acc,\n",
    "        \"erreur\": err\n",
    "    }\n",
    "\n",
    "\n",
    "def eval_cv(model, model_name, X, y, cv_splits=5):\n",
    "    \"\"\"\n",
    "    Évalue un modèle en validation croisée stratifiée k-fold.\n",
    "\n",
    "    Utilise cross_val_predict pour obtenir, pour chaque observation,\n",
    "    une prédiction issue d'un modèle entraîné sans cette observation.\n",
    "    On peut ainsi construire :\n",
    "      - une matrice de confusion globale (en cumulant tous les plis)\n",
    "      - une accuracy globale (proportion d'exemples bien classés)\n",
    "\n",
    "    Retourne un dictionnaire avec les métriques principales.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n================= {model_name} — VALIDATION CROISÉE {cv_splits}-fold =================\")\n",
    "\n",
    "    # Définition de la stratégie de validation croisée (stratifiée)\n",
    "    cv = StratifiedKFold(\n",
    "        n_splits=cv_splits,   # nombre de plis k\n",
    "        shuffle=True,         # on mélange avant de découper\n",
    "        random_state=0        # graine pour reproductibilité\n",
    "    )\n",
    "\n",
    "    # cross_val_predict renvoie, pour chaque exemple, la prédiction obtenue\n",
    "    # quand cet exemple se trouve dans le pli de test (jamais dans le train)\n",
    "    y_pred_cv = cross_val_predict(\n",
    "        model, X, y,\n",
    "        cv=cv,\n",
    "        n_jobs=-1             # parallélisation si possible\n",
    "    )\n",
    "\n",
    "    # Accuracy globale (équivalent à la moyenne des accuracies sur les k plis,\n",
    "    # tous les plis ayant pratiquement la même taille)\n",
    "    acc_cv = accuracy_score(y, y_pred_cv)\n",
    "    err_cv = 1.0 - acc_cv\n",
    "\n",
    "    print(f\"\\nAccuracy (CV {cv_splits}-fold)      : {acc_cv:.4f}\")\n",
    "    print(f\"Taux d'erreur (CV {cv_splits}-fold) : {err_cv:.4f}\\n\")\n",
    "\n",
    "    # Rapport de classification global (toutes les prédictions cumulées)\n",
    "    print(\"Rapport de classification (validation croisée) :\")\n",
    "    print(classification_report(\n",
    "        y,\n",
    "        y_pred_cv,\n",
    "        target_names=[\"Bénigne (0)\", \"Maligne (1)\"]\n",
    "    ))\n",
    "\n",
    "    # Matrice de confusion globale (en cumulant tous les plis)\n",
    "    cm_cv = confusion_matrix(y, y_pred_cv, labels=[0, 1])\n",
    "\n",
    "    print(\"Matrice de confusion globale (CV) :\")\n",
    "    print(cm_cv)\n",
    "\n",
    "    # Détail des faux positifs / faux négatifs\n",
    "    decrire_matrice_confusion(cm_cv, labels=[0, 1])\n",
    "\n",
    "    # Affichage graphique de la matrice de confusion globale\n",
    "    disp = ConfusionMatrixDisplay(\n",
    "        confusion_matrix=cm_cv,\n",
    "        display_labels=[\"Bénigne (0)\", \"Maligne (1)\"]\n",
    "    )\n",
    "    fig, ax = plt.subplots()\n",
    "    disp.plot(ax=ax, values_format=\"d\")\n",
    "    plt.title(f\"Matrice de confusion — {model_name} (CV {cv_splits}-fold)\")\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"modèle\": model_name,\n",
    "        \"stratégie\": f\"CV-{cv_splits}\",\n",
    "        \"accuracy\": acc_cv,\n",
    "        \"erreur\": err_cv\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dcfce6",
   "metadata": {},
   "source": [
    "#  Cellule 5 : Liste pour stocker les résultats de tous les modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd40b2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On gardera ici un dictionnaire de résultats par (modèle, stratégie)\n",
    "resultats = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
